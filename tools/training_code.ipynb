{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd02647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83",
   "display_name": "Python 3.9.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from context import src\n",
    "from src.model.data_handler import save_datasets, load_datasets\n",
    "from src.model.model import RubiksModel\n",
    "from src.model.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dataset with 2,000,000 random scrambled cubes between 5 and 21 solves\n",
    "# First create a folder called \"ignore\". This will not be uploaded to github\n",
    "datapath = save_datasets(\"ignore/\", 2000000, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"\" # Use this if you are loading in an existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_dimensions = ((2000, 1500, 500), (2500, 2500, 1000), (3000, 2500, 1500))\n",
    "dropout_rates = [0, 0.1, 0.2]\n",
    "learning_rates = [0.0008, 0.0005, 0.0002]\n",
    "counter = 0 # set this to the next epoch after a crash and uncomment the next line\n",
    "            # if it does crash, also edit the above lists to ensure you aren't retraining \n",
    "            # models you don't have to. \n",
    "last_saved = -1\n",
    "results = {} if last_saved == -1 else torch.load(\"models/results_{}\".format(last_saved))\n",
    "\n",
    "for internal_dimension in internal_dimensions:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for learning_rate in learning_rates:\n",
    "            if counter > last_saved:\n",
    "                model = RubiksModel(internal_dimensions=internal_dimension, dropout_rate=dropout_rate, activation=nn.ReLU)\n",
    "                train_acc, train_loss, valid_acc, valid_loss = train(model, learning_rate=learning_rate, num_epochs=10, data_path=datapath, savepath=\"models/\")\n",
    "                results[(internal_dimension, dropout_rate, learning_rate)] = (train_acc, train_loss, valid_acc, valid_loss)\n",
    "                torch.save(results, \"models/results_{}\".format(counter))\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 (800, 800, 320, 0.3, 0.001) 2.23334683719635\n1 (800, 800, 320, 0.3, 0.005) 2.285426057357788\n2 (800, 800, 320, 0.3, 0.0001) 2.2638489223098754\n3 (800, 800, 320, 0.5, 0.001) 2.2661198281478883\n4 (800, 800, 320, 0.5, 0.005) 2.402115195579529\n5 (800, 800, 320, 0.5, 0.0001) 2.2866964318466185\n6 (800, 800, 320, 0.7, 0.001) 2.5852361255645753\n7 (800, 800, 320, 0.7, 0.005) 2.8207526250839234\n8 (800, 800, 320, 0.7, 0.0001) 2.607953653755188\n9 (1500, 1500, 500, 0.3, 0.001) 2.2250081211471557\n10 (1500, 1500, 500, 0.3, 0.005) 2.2549870809173584\n11 (1500, 1500, 500, 0.3, 0.0001) 2.255958892555237\n12 (1500, 1500, 500, 0.5, 0.001) 2.2317089212417605\n13 (1500, 1500, 500, 0.5, 0.005) 2.3613300968551636\n14 (1500, 1500, 500, 0.5, 0.0001) 2.2863158360671996\n15 (1500, 1500, 500, 0.7, 0.001) 2.457374667930603\n16 (1500, 1500, 500, 0.7, 0.005) 2.7665268840408324\n17 (1500, 1500, 500, 0.7, 0.0001) 2.5456523714065553\n18 (2500, 2500, 1000, 0.3, 0.001) 2.21583204536438\n19 (2500, 2500, 1000, 0.3, 0.005) 2.2508458718109132\n20 (2500, 2500, 1000, 0.3, 0.0001) 2.2499049685287478\n21 (2500, 2500, 1000, 0.5, 0.001) 2.230669603843689\n22 (2500, 2500, 1000, 0.5, 0.005) 2.3330562689208985\n23 (2500, 2500, 1000, 0.5, 0.0001) 2.311740138282776\n24 (2500, 2500, 1000, 0.7, 0.001) 2.3351538335418702\n25 (2500, 2500, 1000, 0.7, 0.005) 2.6741801923370363\n26 (2500, 2500, 1000, 0.7, 0.0001) 2.4908431044006347\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "((2500, 2500, 1000), 0.2, 0.0008)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cecc40dba990>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0008\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Acc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ((2500, 2500, 1000), 0.2, 0.0008)"
     ]
    }
   ],
   "source": [
    "# results = torch.load(r\"D:\\Unversity of Toronto\\Courses\\Year 3\\APS360 (Applied Fundamentals of Machine Learning)\\Project\\results\\modelsround2\\results_26\")\n",
    "results = torch.load(r\"D:\\Unversity of Toronto\\Courses\\Year 3\\APS360 (Applied Fundamentals of Machine Learning)\\Project\\results\\results_t1\\results26\")\n",
    "\n",
    "for i, item in enumerate(results):\n",
    "    train_acc, train_loss, valid_acc, valid_loss = results[item]\n",
    "    print(i, item, np.min(valid_loss))\n",
    "\n",
    "train_acc, train_loss, valid_acc, valid_loss = results[((2500, 2500, 1000), 0.2, 0.0008)]\n",
    "\n",
    "plt.plot(np.arange(len(train_acc)), train_acc, np.arange(len(train_acc)), valid_acc, label=\"Acc\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(np.arange(len(train_loss)), train_loss, np.arange(len(train_loss)), valid_loss, label=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}